{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: NumPy array Indexing/Slicing\n",
    "\n",
    "**Ex 1.1:** Load the \"iris.csv\" using the appropriate method for this file type (use the new functions from the package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_df = pd.read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\iris\\iris.csv')\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.2:** Select the penultimate independent variable. What is the dimension of the resulting array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the resulting array: (150,)\n"
     ]
    }
   ],
   "source": [
    "penultimate_variable = iris_df.iloc[:, -2]\n",
    "print(\"Dimension of the resulting array:\", penultimate_variable.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.3:** Select the last 10 samples from the iris dataset. What is the mean of the last 10 samples for each\n",
    "independent variable/feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the last 10 samples for each independent variable/feature:\n",
      " sepal_length    6.45\n",
      "sepal_width     3.03\n",
      "petal_length    5.33\n",
      "petal_width     2.17\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "last10 = iris_df.iloc[-10:, :-1]\n",
    "mean_last10 = last10.mean()\n",
    "print(\"Mean of the last 10 samples for each independent variable/feature:\\n\", mean_last10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.4:** Select all samples from the dataset with values less than or equal to 6 for all independent variables/features. How many samples do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with values less than or equal to 6 for all independent variables/features: 89\n"
     ]
    }
   ],
   "source": [
    "filtered_samples = iris_df[(iris_df.iloc[:, :-1] <= 6).all(axis=1)]\n",
    "num_samples = filtered_samples.shape[0]\n",
    "print(\"Number of samples with values less than or equal to 6 for all independent variables/features:\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.5:** Select all samples with a class/label different from 'Iris-setosa'. How many samples do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with a class/label different from 'Iris-setosa': 100\n"
     ]
    }
   ],
   "source": [
    "samples = iris_df[iris_df['class'] != 'Iris-setosa']\n",
    "num_samples = samples = iris_df[iris_df['class'] != 'Iris-setosa'].shape[0]\n",
    "print(\"Number of samples with a class/label different from 'Iris-setosa':\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2:** \n",
    "\n",
    "Examples of how to use the fillna, dropna and remove_by_index methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from si.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "X: [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [nan 3.  5.6 nan]]\n",
      "y: ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "#Turning the iris dataset into a Dataset object and adding a row with NaN values\n",
    "X = last10.values\n",
    "new_row = np.array([np.nan, 3., 5.6, np.nan])\n",
    "X = np.vstack([X, new_row])\n",
    "y = iris_df.iloc[-10:, -1].values\n",
    "new_y = np.array(['Iris-setosa'])\n",
    "y = np.append(y, new_y)\n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "print(\"Original Dataset:\")\n",
    "print(\"X:\", dataset.X)\n",
    "print(\"y:\", dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains missing values.\n"
     ]
    }
   ],
   "source": [
    "if np.isnan(dataset.X).any() :\n",
    "    print(\"The dataset contains missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copy: \n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [nan 3.  5.6 nan]]\n",
      "['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n",
      "\n",
      "Size of X before dropna: 10\n",
      "Size of y before dropna: 10\n",
      "\n",
      "Size of X after dropna:\n",
      " 10\n",
      "Size of y after dropna:\n",
      " 10\n",
      "\n",
      "X after dropna:\n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y after dropna: ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "#Making a copy of the dataset\n",
    "X_copy = np.copy(dataset.X)\n",
    "y_copy = np.copy(dataset.y) \n",
    "dataset_copy = Dataset(X_copy, y_copy)\n",
    "\n",
    "print(\"Dataset copy: \\n\", dataset_copy.X)\n",
    "print(dataset_copy.y)\n",
    "\n",
    "\n",
    "\n",
    "# Removing samples with missing values using the dropna method\n",
    "cleaned_dataset = dataset_copy.dropna()\n",
    "print(\"\\nSize of X before dropna:\", len(dataset_copy.X))\n",
    "print(\"Size of y before dropna:\", len(dataset_copy.y))\n",
    "\n",
    "print(\"\\nSize of X after dropna:\\n\", len(cleaned_dataset.X))\n",
    "print(\"Size of y after dropna:\\n\", len(cleaned_dataset.y))\n",
    "\n",
    "print(\"\\nX after dropna:\\n\", cleaned_dataset.X)\n",
    "print(\"y after dropna:\", cleaned_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X after fillna:\n",
      " [[   6.7    3.1    5.6    2.4]\n",
      " [   6.9    3.1    5.1    2.3]\n",
      " [   5.8    2.7    5.1    1.9]\n",
      " [   6.8    3.2    5.9    2.3]\n",
      " [   6.7    3.3    5.7    2.5]\n",
      " [   6.7    3.     5.2    2.3]\n",
      " [   6.3    2.5    5.     1.9]\n",
      " [   6.5    3.     5.2    2. ]\n",
      " [   6.2    3.4    5.4    2.3]\n",
      " [   5.9    3.     5.1    1.8]\n",
      " [1111.     3.     5.6 1111. ]]\n",
      "y after fillna:\n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values with the mean of the corresponding feature using the fillna method\n",
    "filled_dataset = dataset.fillna(1111.0)\n",
    "print(\"X after fillna:\\n\", filled_dataset.X)\n",
    "print(\"y after fillna:\\n\", filled_dataset.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the modified dataset: (10, 4)\n",
      "X after remove_by_index:\n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y after remove_by_index:\n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# Removing the last sample using the remove_by_index method\n",
    "removed_dataset = filled_dataset.remove_by_index(-1)\n",
    "print(\"Size of the modified dataset:\", dataset.X.shape)\n",
    "print(\"X after remove_by_index:\\n\", removed_dataset.X)\n",
    "print(\"y after remove_by_index:\\n\", removed_dataset.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 3:** Implementing SelectPercentile\n",
    "\n",
    "Testing the SelectPercentile class using the \"iris.csv\" dataset \n",
    "\n",
    "- This class allows for the selection of features according to a given percentile, therefore we will see if the sizes of the datasets and features of the new dataframe correspond to the expected.\n",
    "- Expected results: in a dataset with a total number of 4 features such as the iris.cvs, with a percentile of 25% and 50% there should be a selection of 1 and 2 features respectively. Additionally the number of samples must not change and the selection should be done considering the highest values of F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.statistics.f_classification import f_classification\n",
    "from si.feature_selection.select_percentile import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (150, 4)\n",
      "Original number of features: 4\n",
      "\n",
      "Selector percentile: 25\n",
      "Transformed dataset shape: (150, 1)\n",
      "Selected features: ['petal_length']\n",
      "\n",
      "Selector percentile: 50.0\n",
      "Transformed dataset shape: (150, 2)\n",
      "Selected features: ['petal_width', 'petal_length']\n",
      "\n",
      "Selector percentile: 100\n",
      "Transformed dataset shape: (150, 4)\n",
      "Selected features: ['sepal_width', 'sepal_length', 'petal_width', 'petal_length']\n"
     ]
    }
   ],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)\n",
    "\n",
    "selectors = [ SelectPercentile(percentile=25), SelectPercentile(percentile=50.0), SelectPercentile(percentile=100) ]\n",
    "\n",
    "print(\"Original dataset shape:\", iris_dataset.X.shape)\n",
    "print(\"Original number of features:\", len(iris_dataset.features))\n",
    "for selector in selectors:\n",
    "    selector.fit(iris_dataset)\n",
    "    transformed_dataset = selector.transform(iris_dataset)\n",
    "    print(\"\\nSelector percentile:\", selector.percentile)\n",
    "    print(\"Transformed dataset shape:\", transformed_dataset.X.shape)\n",
    "    print(\"Selected features:\", transformed_dataset.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], dtype='object')\n",
      "Selector percentile: 25\n",
      "F values: [ 119.26450218   47.3644614  1179.0343277   959.32440573]\n",
      "p values: [1.66966919e-31 1.32791652e-16 3.05197580e-91 4.37695696e-85]\n",
      "Selector percentile: 50.0\n",
      "F values: [ 119.26450218   47.3644614  1179.0343277   959.32440573]\n",
      "p values: [1.66966919e-31 1.32791652e-16 3.05197580e-91 4.37695696e-85]\n",
      "Selector percentile: 100\n",
      "F values: [ 119.26450218   47.3644614  1179.0343277   959.32440573]\n",
      "p values: [1.66966919e-31 1.32791652e-16 3.05197580e-91 4.37695696e-85]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original features:\", iris_dataset.features)\n",
    "for selector in selectors:\n",
    "    selector.fit(iris_dataset)\n",
    "    transformed_dataset = selector.transform(iris_dataset)\n",
    "    print(\"Selector percentile:\", selector.percentile)\n",
    "    print(\"F values:\", selector.F)\n",
    "    print(\"p values:\", selector.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the features with the highest F values (smallest p values) were the ones selected in the selectors with the different percentiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 4:** Cosine Distance\n",
    "\n",
    "Comparison with the already existent cosine_distances from the sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine implementation: [0.         0.02536815]\n",
      "Sklearn function: [[0.         0.02536815]]\n"
     ]
    }
   ],
   "source": [
    "from si.statistics.cosine_distance import cosine_distance\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "distance = cosine_distance(x, y)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "sklearn_distance = cosine_distances(x.reshape(1, -1), y)\n",
    "\n",
    "print(\"Cosine implementation:\", distance)\n",
    "print(\"Sklearn function:\", sklearn_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercício 5:** PCA\n",
    "\n",
    "Testing the PCA class in a jupyter notebook using the iris.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.decomposition.pca import PCA\n",
    "from sklearn.decomposition import PCA as PCA_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class\n",
    "np.random.seed(5)\n",
    "pca = PCA(n_components=2)\n",
    "pca._fit(iris_dataset.X)\n",
    "X_transformed = pca._transform(iris_dataset.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class from scikit-learn\n",
    "np.random.seed(5)\n",
    "pca_sklearn = PCA_sklearn(n_components=2)\n",
    "pca_sklearn.fit(iris_dataset.X)\n",
    "X_transformed_sklearn = pca_sklearn.transform(iris_dataset.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [0.92461621 0.05301557]\n",
      "Explained variance by PCA from scikit-learn: [4.22484077 0.24224357]\n",
      "\n",
      "Transformed data structure: (150, 2)\n",
      "Transformed data structure by PCA from scikit-learn: (150, 2)\n",
      "\n",
      "First five lines of the Transformed data:\n",
      " [[-2.68420713 -0.32660731]\n",
      " [-2.71539062  0.16955685]\n",
      " [-2.88981954  0.13734561]\n",
      " [-2.7464372   0.31112432]\n",
      " [-2.72859298 -0.33392456]]\n",
      "First five lines of the Transformed data by PCA from scikit-learn:\n",
      " [[-2.68420713  0.32660731]\n",
      " [-2.71539062 -0.16955685]\n",
      " [-2.88981954 -0.13734561]\n",
      " [-2.7464372  -0.31112432]\n",
      " [-2.72859298  0.33392456]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained variance:\", pca.explained_variance)\n",
    "print(\"Explained variance by PCA from scikit-learn:\", pca_sklearn.explained_variance_)\n",
    "\n",
    "print(\"\\nTransformed data structure:\", X_transformed.shape)\n",
    "print(\"Transformed data structure by PCA from scikit-learn:\", X_transformed_sklearn.shape)\n",
    "\n",
    "print (\"\\nFirst five lines of the Transformed data:\\n\", X_transformed[:5])\n",
    "print(\"First five lines of the Transformed data by PCA from scikit-learn:\\n\", X_transformed_sklearn[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class and normalizing the data\n",
    "pca_norm = PCA(n_components=2)\n",
    "pca_norm._fit(iris_dataset.X, normalization=True)\n",
    "X_transformed_norm = pca_norm._transform(iris_dataset.X, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class and normalizing the data using the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "np.random.seed(5)\n",
    "X_scaled = StandardScaler().fit_transform(iris_dataset.X)\n",
    "pca_scaled = PCA(n_components=2)\n",
    "pca_scaled._fit(X_scaled)\n",
    "X_transformed_scaled = pca_scaled._transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [0.72770452 0.23030523]\n",
      "Explained variance using the StandardScaler: [0.72770452 0.23030523]\n",
      "\n",
      "Transformed data structure (normalized): (150, 2)\n",
      "Transformed data structure using the StandardScaler: (150, 2)\n",
      "\n",
      "First five lines of the Transformed data (normalized):\n",
      " [[-2.44159388 -0.02095745]\n",
      " [-2.41439075  0.51628447]\n",
      " [-2.62966145  0.40774632]\n",
      " [-2.53931232  0.53331485]\n",
      " [-2.52016653 -0.07628126]]\n",
      "First five lines of the Transformed data using the StandardScaler:\n",
      " [[-2.26454173 -0.5057039 ]\n",
      " [-2.0864255   0.65540473]\n",
      " [-2.36795045  0.31847731]\n",
      " [-2.30419716  0.57536771]\n",
      " [-2.38877749 -0.6747674 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained variance:\", pca_norm.explained_variance)\n",
    "print(\"Explained variance using the StandardScaler:\", pca_scaled.explained_variance)\n",
    "\n",
    "print(\"\\nTransformed data structure (normalized):\", X_transformed_norm.shape)\n",
    "print(\"Transformed data structure using the StandardScaler:\", X_transformed_scaled.shape)\n",
    "\n",
    "print (\"\\nFirst five lines of the Transformed data (normalized):\\n\", X_transformed_norm[:5])\n",
    "print(\"First five lines of the Transformed data using the StandardScaler:\\n\", X_transformed_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 6:** Implementing stratified splitting\n",
    "\n",
    "Test the \"stratified_train_test_split\" function with the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.model_selection.split import train_test_split, stratified_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(iris_dataset)\n",
    "train_strat, test_strat = stratified_train_test_split(iris_dataset)\n",
    "train_strat_50, test_strat_50 = stratified_train_test_split(iris_dataset, test_size=0.5)\n",
    "train_strat_seed, test_strat_seed= stratified_train_test_split(iris_dataset, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test dataset sizes: (120, 4) (30, 4)\n",
      "Train and Test dataset sizes (stratified): (120, 4) (30, 4)\n",
      "Train and Test dataset sizes (stratified), with half used for training: (75, 4) (75, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and test dataset sizes:\", train.X.shape, test.X.shape)\n",
    "print(\"Train and Test dataset sizes (stratified):\", train_strat.X.shape, test_strat.X.shape)\n",
    "print(\"Train and Test dataset sizes (stratified), with half used for training:\", train_strat_50.X.shape, test_strat_50.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size, with default seed:\n",
      " [[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]]\n",
      "Test dataset size, with default seed:\n",
      " [[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]]\n",
      "\n",
      "Train dataset size (stratified), with default seed:\n",
      " [[4.8 3.  1.4 0.1]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n",
      "Test dataset size (stratified), with default seed:\n",
      " [[4.3 3.  1.1 0.1]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.5 1.4 0.3]]\n",
      "\n",
      "Train adataset size (stratified), with seed=5:\n",
      " [[5.4 3.4 1.5 0.4]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.3 1.7 0.5]]\n",
      "Test dataset size (stratified), with seed=5:\n",
      " [[4.4 3.2 1.3 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.2 3.4 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size, with default seed:\\n\", train.X[:5])\n",
    "print(\"Test dataset size, with default seed:\\n\", test.X[:5])\n",
    "\n",
    "print(\"\\nTrain dataset size (stratified), with default seed:\\n\", train_strat.X[:5])\n",
    "print(\"Test dataset size (stratified), with default seed:\\n\", test_strat.X[:5])\n",
    "\n",
    "print(\"\\nTrain adataset size (stratified), with seed=5:\\n\", train_strat_seed.X[:5])\n",
    "print(\"Test dataset size (stratified), with seed=5:\\n\", test_strat_seed.X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 7:** mplementing the KNNRegressor with RMSE\n",
    "\n",
    "7.3. Test the \"KNNRegressor\" class using the \"cpu.csv\" dataset (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.metrics.rmse import rmse\n",
    "from si.models.knn_regressor import KNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 198,  269,  220,  172,  132,  318,  367,  489,  636, 1144,   38,\n",
       "         40,   92,  138,   10,   35,   19,   28,   31,  120,   30,   33,\n",
       "         61,   76,   23,   69,   33,   27,   77,   27,  274,  368,   32,\n",
       "         63,  106,  208,   20,   29,   71,   26,   36,   40,   52,   60,\n",
       "         72,   72,   18,   20,   40,   62,   24,   24,  138,   36,   26,\n",
       "         60,   71,   12,   14,   20,   16,   22,   36,  144,  144,  259,\n",
       "         17,   26,   32,   32,   62,   64,   22,   36,   44,   50,   45,\n",
       "         53,   36,   84,   16,   38,   38,   16,   22,   29,   40,   35,\n",
       "        134,   66,  141,  189,   22,  132,  237,  465,  465,  277,  185,\n",
       "          6,   24,   45,    7,   13,   16,   32,   32,   11,   11,   18,\n",
       "         22,   37,   40,   34,   50,   76,   66,   24,   49,   66,  100,\n",
       "        133,   12,   18,   20,   27,   45,   56,   70,   80,  136,   16,\n",
       "         26,   32,   45,   54,   65,   30,   50,   40,   62,   60,   50,\n",
       "         66,   86,   74,   93,  110,  143,  105,  214,  277,  370,  510,\n",
       "        214,  326,  510,    8,   12,   17,   21,   24,   34,   42,   46,\n",
       "         51,  116,  100,  140,  212,   25,   30,   41,   25,   50,   50,\n",
       "         30,   32,   38,   60,  109,    6,   11,   22,   33,   58,  130,\n",
       "         75,  113,  188,  173,  248,  405,   70,  114,  208,  307,  397,\n",
       "        915, 1150,   12,   14,   18,   21,   42,   46,   52,   67,   45])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu = read_csv('../datasets/cpu/cpu.csv', features=True, label=True)\n",
    "\n",
    "cpu.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test dataset sizes: (192, 6) (17, 6)\n",
      "Train dataset:\n",
      " [[ 480   96  512    0    1    1]\n",
      " [ 240  512 1000    8    1    3]\n",
      " [1100  512 1500    0    1    1]\n",
      " [ 112 1000 1000    0    1    4]\n",
      " [ 350   64   64    0    1    4]]\n",
      "Test dataset:\n",
      " [[ 180  262 4000    0    1    3]\n",
      " [ 330 1000 2000    0    1    2]\n",
      " [ 900 1000 4000    4    1    2]\n",
      " [ 800  256 8000    0    1    4]\n",
      " [ 330 1000 4000    0    3    6]]\n"
     ]
    }
   ],
   "source": [
    "train_cpu, test_cpu = stratified_train_test_split(cpu, test_size=0.3, random_state=5)\n",
    "print(\"Train and test dataset sizes:\", train_cpu.X.shape, test_cpu.X.shape)\n",
    "print(\"Train dataset:\\n\", train_cpu.X[:5])\n",
    "print(\"Test dataset:\\n\", test_cpu.X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18., 24., 22., 14., 38., 56., 32., 24., 34., 34., 44., 16., 25.,\n",
       "       52., 25., 52., 70.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn regressor with k=1 + stratified split\n",
    "kmeans = KNNRegressor()\n",
    "kmeans.fit(train_cpu)\n",
    "predictions = kmeans.predict(test_cpu)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.37029191462748"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(test_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21.6,  21.6,  33.4,  17.6,  34.2,  67.8,  41.6,  31.6,  33. ,\n",
       "        61.2, 145.8, 145.8,  23.4,  56.2,  24. ,  59.2,  67.8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn regressor with k=5 + stratified split\n",
    "kmeans = KNNRegressor(k=5)\n",
    "kmeans.fit(train_cpu)\n",
    "predictions = kmeans.predict(test_cpu)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.79434793607335"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(test_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing RidgeRegression**\n",
    "\n",
    "1. Use thedataset cpu.csv\n",
    "2. Divide the dataset into train and test sets\n",
    "3. Train the model. Which score do you get? And the cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 12721.613689859541\n",
      "Cost: 6429.521624869436\n"
     ]
    }
   ],
   "source": [
    "from si.models.linear_regression import RidgeRegression\n",
    "\n",
    "cpu_dataset = read_csv('../datasets/cpu/cpu.csv', features=True, label=True)\n",
    "\n",
    "cpu_train, cpu_test = train_test_split(cpu_dataset, test_size=0.2, random_state=5)\n",
    "\n",
    "ridge_model = RidgeRegression()\n",
    "ridge_model.fit(cpu_train)\n",
    "predictions = ridge_model.predict(cpu_test)\n",
    "test_score = ridge_model.score(cpu_test)\n",
    "test_cost = ridge_model.cost(cpu_test)\n",
    "\n",
    "print(\"Score:\", test_score)\n",
    "print(\"Cost:\", test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2962.5932159158297\n",
      "Cost: 1889.033227942686\n"
     ]
    }
   ],
   "source": [
    "cpu_train_strat, cpu_test_strat = stratified_train_test_split(cpu_dataset, test_size=0.2, random_state=5)\n",
    "\n",
    "ridge_model2 = RidgeRegression()\n",
    "ridge_model2.fit(cpu_train_strat)\n",
    "predictions_strat = ridge_model2.predict(cpu_test_strat)\n",
    "test_score_strat = ridge_model2.score(cpu_test_strat)\n",
    "test_cost_strat = ridge_model2.cost(cpu_test_strat)\n",
    "\n",
    "print(\"Score:\", test_score_strat)\n",
    "print(\"Cost:\", test_cost_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1107.5728118060001\n",
      "Cost: 1199.0057277113979\n"
     ]
    }
   ],
   "source": [
    "ridge_model2 = RidgeRegression(alpha=0.1)\n",
    "ridge_model2.fit(cpu_train_strat)\n",
    "predictions_strat = ridge_model2.predict(cpu_test_strat)\n",
    "test_score_strat = ridge_model2.score(cpu_test_strat)\n",
    "test_cost_strat = ridge_model2.cost(cpu_test_strat)\n",
    "\n",
    "print(\"Score:\", test_score_strat)\n",
    "print(\"Cost:\", test_cost_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TestingRidgeRegression and LogisticRegression** using the breast-bin.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.models.logistic_regression import LogisticRegression\n",
    "\n",
    "bb_dataset = read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\breast_bin\\breast-bin.csv', features=True, label=True)\n",
    "\n",
    "bb_train, bb_test = stratified_train_test_split(bb_dataset, test_size=0.3, random_state=5)\n",
    "\n",
    "# Ridge regression model\n",
    "ridge_model = RidgeRegression(alpha=0.1)\n",
    "ridge_model.fit(bb_train)\n",
    "predictions_R = ridge_model.predict(bb_test)\n",
    "test_score_R = ridge_model.score(bb_test)\n",
    "test_cost_R = ridge_model.cost(bb_test)\n",
    "\n",
    "# Logistic regression model\n",
    "logistic_model = LogisticRegression(alpha=0.1)\n",
    "logistic_model.fit(bb_train)\n",
    "predictions_L = logistic_model.predict(bb_test)\n",
    "test_score_L = logistic_model.score(bb_test)\n",
    "test_cost_L = logistic_model.cost(bb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for the Ridge Regression: 0.03588477223004671\n",
      "Cost for the Ridge Regression: 0.018055436565692178\n",
      "\n",
      "Score for the Logistic Regression: 0.9760765550239234\n",
      "Cost for the Logistic Regression: 1.445038601820536\n"
     ]
    }
   ],
   "source": [
    "print(\"Score for the Ridge Regression:\", test_score_R)\n",
    "print(\"Cost for the Ridge Regression:\", test_cost_R)\n",
    "\n",
    "print(\"\\nScore for the Logistic Regression:\", test_score_L)\n",
    "print(\"Cost for the Logistic Regression:\", test_cost_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 9.2:** Testing Random Forest\n",
    "1.Use the iris.csv dataset\n",
    "\n",
    "2.Split the data into train and test sets\n",
    "\n",
    "3.Create the RandomForestClassifier model\n",
    "\n",
    "4.Train the model. What is the score of the model on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.models.random_forest_classifier import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<si.models.random_forest_classifier.RandomForestClassifier at 0x123e597f020>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)\n",
    "train_iris, test_iris = stratified_train_test_split(iris_dataset, test_size=0.2, random_state=5)\n",
    "\n",
    "rf_model = RandomForestClassifier(seed=5)\n",
    "rf_model.fit(train_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "predictions_rf = rf_model.predict(test_iris)\n",
    "test_score_rf = rf_model.score(test_iris)\n",
    "\n",
    "print(\"Score:\", test_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 10.2:** Testing the StackingClassifier model\n",
    "\n",
    "1.Use the breast-bin.csv dataset\n",
    "\n",
    "2.Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10 = read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\breast_bin\\breast-bin.csv', features=True, label=True)\n",
    "\n",
    "train_10, test_10 = stratified_train_test_split(dataset_10, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Create a KNNClassifier model\n",
    "\n",
    "4.Create a LogisticRegression model\n",
    "\n",
    "5.Create a DecisionTree model\n",
    "\n",
    "6.Create a second KNNClassifier model (final model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "\n",
    "knn_model = KNNClassifier()\n",
    "log_reg_model = LogisticRegression()\n",
    "decision_tree_model = DecisionTreeClassifier(max_depth=5)\n",
    "final_model = KNNClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Create a StackingClassifier model using the previous classifiers. The second KNNClassifier model must be used as the final model.\n",
    "\n",
    "8.Train the StackingClassifier model. What is the score of the model on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9827586206896551\n"
     ]
    }
   ],
   "source": [
    "from si.ensemble.stacking_classifier import StackingClassifier\n",
    "\n",
    "stacking_model = StackingClassifier(models=[knn_model, log_reg_model, decision_tree_model], final_model=final_model)\n",
    "stacking_model.fit(train_10)\n",
    "test_score_stacking = stacking_model.score(test_10)\n",
    "\n",
    "print(\"Score:\", test_score_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 11:** Implementing the randomized_search_cv function\n",
    "1. Use the breast-bin.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "si.data.dataset.Dataset"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_bin = read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\breast_bin\\breast-bin.csv', features=True, label=True)\n",
    "\n",
    "type(breast_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a LogisticRegression model\n",
    "\n",
    "3. Perform a randomized search with the following hyperparameter distributions:\n",
    "* l2_penalty: distribution between 1 and 10 with 10 equal intervals (e.g., np.linspace(1, 10, 10))\n",
    "* alpha: distribution between 0.001 and 0.0001 with 100 equal intervals (e.g., np.linspace(0.001, 0.0001, 100))\n",
    "* max_iter: distribution between 1000 and 2000 with 200 equal intervals (e.g., np.linspace(1000, 2000, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "from si.model_selection.cross_validate import k_fold_cross_validation \n",
    "\n",
    "model_log = LogisticRegression()\n",
    "\n",
    "hyperparameter_grid = {'l2_penalty': np.linspace(1, 10, 10) ,'alpha': np.linspace(0.001, 0.0001, 100), \n",
    "                       'max_iter': np.linspace(1000, 2000, 200) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use n_iter=10 and cv=3 folds for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = randomized_search_cv(model=model_log, \n",
    "                                    dataset=breast_bin, \n",
    "                                    hyperparameter_grid=hyperparameter_grid, \n",
    "                                    n_iter=10, cv=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which scores do you obtain? What are the best score and best hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "0.9669540229885057\n",
      "0.9655172413793104\n",
      "0.9669540229885057\n",
      "0.9655172413793104\n",
      "0.9669540229885057\n",
      "0.9669540229885057\n",
      "0.9669540229885057\n",
      "0.9655172413793104\n",
      "0.9669540229885057\n",
      "0.9669540229885057\n",
      "\n",
      "Best hyperparameters: {'l2_penalty': np.float64(3.0), 'alpha': np.float64(0.00016363636363636363), 'max_iter': np.float64(1824.1206030150754)}\n",
      "Best score: 0.9669540229885057\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores:\")\n",
    "for score in dict_results['scores']:\n",
    "    print (score)\n",
    "print(\"\\nBest hyperparameters:\", dict_results['best_hyperparameters'])\n",
    "print(\"Best score:\", dict_results['best_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 12:** Testing the Dropout layer\n",
    "\n",
    "12.2. Test the layer with a random input and check if the output shows the desired behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during training:\n",
      "[[1.71681479 1.76990811 0.         1.01098369 0.86416605 0.\n",
      "  0.57001427 1.72527462 0.         0.27492492]\n",
      " [0.         0.         0.         1.03489279 0.         0.\n",
      "  1.66741543 1.27854196 0.         0.        ]\n",
      " [1.6331077  0.         1.6788027  0.         0.85633386 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.21595473 0.21243207 0.41268677 0.02426719 0.         0.72948484\n",
      "  0.         1.14853787 1.26995664 1.92199609]\n",
      " [0.         0.         1.74917661 0.8936013  1.78355493 0.\n",
      "  0.         0.25720193 0.         1.64398588]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.layers import Dropout\n",
    "\n",
    "dropout_layer = Dropout(probability=0.5)\n",
    "input_data = np.random.rand(5, 10)\n",
    "dropout_layer.set_input_shape(input_data.shape)  \n",
    "\n",
    "# Forward propagation during training --> expected to have some zeros due to the mask\n",
    "output_train = dropout_layer.forward_propagation(input_data, training=True)\n",
    "print(\"Output during training:\")\n",
    "print(output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during inference is the same as the input.\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation during inference --> shoulb the same as the input\n",
    "output_inference = dropout_layer.forward_propagation(input_data, training=False)\n",
    "if (output_inference == input_data).all():\n",
    "    print(\"Output during inference is the same as the input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input error during backpropagation:\n",
      "[[1. 1. 0. 1. 1. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 0. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Backward propagation --> by using a dummy error for testing, we should see some zeros in the input error\n",
    "error = np.ones_like(input_data) \n",
    "input_error = dropout_layer.backward_propagation(error)\n",
    "print(\"Input error during backpropagation:\")\n",
    "print(input_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape is correct\n"
     ]
    }
   ],
   "source": [
    "# Output shape that shpuld be the same as the input shape\n",
    "if dropout_layer.output_shape() == input_data.shape:\n",
    "    print(\"Output shape is correct\")\n",
    "else:\n",
    "    print(\"Something is wrong: Output shape is incorrect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# Parameters of the dropout layer - should be 0\n",
    "if dropout_layer.parameters() == 0:\n",
    "    print(\"Parameters: 0\")\n",
    "else:\n",
    "    print(\"Something is wrong: dropout layers do not have learnable parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
