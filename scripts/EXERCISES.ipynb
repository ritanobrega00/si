{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: NumPy array Indexing/Slicing\n",
    "\n",
    "**Ex 1.1:** Load the \"iris.csv\" using the appropriate method for this file type (use the new functions from the package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_df = pd.read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\iris\\iris.csv')\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.2:** Select the penultimate independent variable. What is the dimension of the resulting array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the resulting array: (150,)\n"
     ]
    }
   ],
   "source": [
    "penultimate_variable = iris_df.iloc[:, -2]\n",
    "print(\"Dimension of the resulting array:\", penultimate_variable.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.3:** Select the last 10 samples from the iris dataset. What is the mean of the last 10 samples for each\n",
    "independent variable/feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the last 10 samples for each independent variable/feature:\n",
      " sepal_length    6.45\n",
      "sepal_width     3.03\n",
      "petal_length    5.33\n",
      "petal_width     2.17\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "last10 = iris_df.iloc[-10:, :-1]\n",
    "mean_last10 = last10.mean()\n",
    "print(\"Mean of the last 10 samples for each independent variable/feature:\\n\", mean_last10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.4:** Select all samples from the dataset with values less than or equal to 6 for all independent variables/features. How many samples do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with values less than or equal to 6 for all independent variables/features: 89\n"
     ]
    }
   ],
   "source": [
    "filtered_samples = iris_df[(iris_df.iloc[:, :-1] <= 6).all(axis=1)]\n",
    "num_samples = filtered_samples.shape[0]\n",
    "print(\"Number of samples with values less than or equal to 6 for all independent variables/features:\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.5:** Select all samples with a class/label different from 'Iris-setosa'. How many samples do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with a class/label different from 'Iris-setosa': 100\n"
     ]
    }
   ],
   "source": [
    "samples = iris_df[iris_df['class'] != 'Iris-setosa']\n",
    "num_samples = samples = iris_df[iris_df['class'] != 'Iris-setosa'].shape[0]\n",
    "print(\"Number of samples with a class/label different from 'Iris-setosa':\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2:** \n",
    "\n",
    "Examples of how to use the fillna, dropna and remove_by_index methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from si.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "X: [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [nan 3.  5.6 nan]]\n",
      "y: ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "#Turning the iris dataset into a Dataset object and adding a row with NaN values\n",
    "X = last10.values\n",
    "new_row = np.array([np.nan, 3., 5.6, np.nan])\n",
    "X = np.vstack([X, new_row])\n",
    "y = iris_df.iloc[-10:, -1].values\n",
    "new_y = np.array(['Iris-setosa'])\n",
    "y = np.append(y, new_y)\n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "print(\"Original Dataset:\")\n",
    "print(\"X:\", dataset.X)\n",
    "print(\"y:\", dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains missing values.\n"
     ]
    }
   ],
   "source": [
    "if np.isnan(dataset.X).any() :\n",
    "    print(\"The dataset contains missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copy: \n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [nan 3.  5.6 nan]]\n",
      "['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n",
      "\n",
      " X after dropna:\n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y after dropna: ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "#Making a copy of the dataset\n",
    "X_copy = np.copy(dataset.X)\n",
    "y_copy = np.copy(dataset.y) \n",
    "dataset_copy = Dataset(X_copy, y_copy)\n",
    "\n",
    "print(\"Dataset copy: \\n\", dataset_copy.X)\n",
    "print(dataset_copy.y)\n",
    "\n",
    "# Removing samples with missing values using the dropna method\n",
    "cleaned_dataset = dataset_copy.dropna()\n",
    "print(\"\\n X after dropna:\\n\", cleaned_dataset.X)\n",
    "print(\"y after dropna:\", cleaned_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X after fillna:\n",
      " [[ 6.7  3.1  5.6  2.4]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]\n",
      " [10.   3.   5.6 10. ]]\n",
      "y after fillna:\n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values with the mean of the corresponding feature using the fillna method\n",
    "filled_dataset = dataset.fillna(10.0)\n",
    "print(\"X after fillna:\\n\", filled_dataset.X)\n",
    "print(\"y after fillna:\\n\", filled_dataset.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the modified dataset: (10, 4)\n",
      "X after remove_by_index:\n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y after remove_by_index:\n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# Removing the last sample using the remove_by_index method\n",
    "removed_dataset = filled_dataset.remove_by_index(-1)\n",
    "print(\"Size of the modified dataset:\", dataset.X.shape)\n",
    "print(\"X after remove_by_index:\\n\", removed_dataset.X)\n",
    "print(\"y after remove_by_index:\\n\", removed_dataset.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 3:** Implementing SelectPercentile\n",
    "\n",
    "Testing the SelectPercentile class using the \"iris.csv\" dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.statistics.f_classification import f_classification\n",
    "from si.feature_selection.select_percentile import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (150, 4)\n",
      "Transformed dataset shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)\n",
    "\n",
    "# Create an instance of the SelectPercentile class\n",
    "selector = SelectPercentile(percentile=50)\n",
    "# Fit the selector to the dataset\n",
    "selector.fit(iris_dataset)\n",
    "\n",
    "# Transform the dataset using the fitted model\n",
    "transformed_dataset = selector.transform(iris_dataset)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original dataset shape:\", iris_dataset.X.shape)\n",
    "print(\"Transformed dataset shape:\", transformed_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F values: [ 119.26450218   47.3644614  1179.0343277   959.32440573]\n",
      "p values: [1.66966919e-31 1.32791652e-16 3.05197580e-91 4.37695696e-85]\n",
      "Original dataset shape: (150, 4)\n",
      "Transformed dataset shape: (150, 2)\n",
      "Original features:\n",
      " Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], dtype='object')\n",
      "Selected features:\n",
      " ['petal_width', 'petal_length']\n"
     ]
    }
   ],
   "source": [
    "selector2 = SelectPercentile()\n",
    "selector2.fit(iris_dataset)\n",
    "transformed_dataset2 = selector.transform(iris_dataset)\n",
    "\n",
    "print(\"F values:\", selector2.F)\n",
    "print(\"p values:\", selector2.p)\n",
    "\n",
    "print(\"Original dataset shape:\", iris_dataset.X.shape)\n",
    "print(\"Transformed dataset shape:\", transformed_dataset2.X.shape)\n",
    "print(\"Original features:\\n\", iris_dataset.features)\n",
    "print(\"Selected features:\\n\", transformed_dataset2.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercício 5:** PCA\n",
    "\n",
    "Testing the PCA class in a jupyter notebook using the iris.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.decomposition.pca import PCA\n",
    "from sklearn.decomposition import PCA as PCA_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class\n",
    "np.random.seed(5)\n",
    "pca = PCA(n_components=2)\n",
    "pca._fit(iris_dataset.X)\n",
    "X_transformed = pca._transform(iris_dataset.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class from scikit-learn\n",
    "np.random.seed(5)\n",
    "pca_sklearn = PCA_sklearn(n_components=2)\n",
    "pca_sklearn.fit(iris_dataset.X)\n",
    "X_transformed_sklearn = pca_sklearn.transform(iris_dataset.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [0.92461621 0.05301557]\n",
      "Explained variance by PCA from scikit-learn: [4.22484077 0.24224357]\n",
      "\n",
      "Transformed data structure: (150, 2)\n",
      "Transformed data structure by PCA from scikit-learn: (150, 2)\n",
      "\n",
      "First five lines of the Transformed data:\n",
      " [[-2.68420713 -0.32660731]\n",
      " [-2.71539062  0.16955685]\n",
      " [-2.88981954  0.13734561]\n",
      " [-2.7464372   0.31112432]\n",
      " [-2.72859298 -0.33392456]]\n",
      "First five lines of the Transformed data by PCA from scikit-learn:\n",
      " [[-2.68420713  0.32660731]\n",
      " [-2.71539062 -0.16955685]\n",
      " [-2.88981954 -0.13734561]\n",
      " [-2.7464372  -0.31112432]\n",
      " [-2.72859298  0.33392456]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained variance:\", pca.explained_variance)\n",
    "print(\"Explained variance by PCA from scikit-learn:\", pca_sklearn.explained_variance_)\n",
    "\n",
    "print(\"\\nTransformed data structure:\", X_transformed.shape)\n",
    "print(\"Transformed data structure by PCA from scikit-learn:\", X_transformed_sklearn.shape)\n",
    "\n",
    "print (\"\\nFirst five lines of the Transformed data:\\n\", X_transformed[:5])\n",
    "print(\"First five lines of the Transformed data by PCA from scikit-learn:\\n\", X_transformed_sklearn[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class and normalizing the data\n",
    "pca_norm = PCA(n_components=2)\n",
    "pca_norm._fit(iris_dataset.X, normalization=True)\n",
    "X_transformed_norm = pca_norm._transform(iris_dataset.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class and normalizing the data using the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "np.random.seed(5)\n",
    "X_scaled = StandardScaler().fit_transform(iris_dataset.X)\n",
    "pca_scaled = PCA(n_components=2)\n",
    "pca_scaled._fit(X_scaled)\n",
    "X_transformed_scaled = pca_scaled._transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [0.72770452 0.23030523]\n",
      "Explained variance using the StandardScaler: [0.72770452 0.23030523]\n",
      "\n",
      "Transformed data structure (normalized): (150, 2)\n",
      "Transformed data structure using the StandardScaler: (150, 2)\n",
      "\n",
      "First five lines of the Transformed data (normalized):\n",
      " [[-2.44159388 -0.02095745]\n",
      " [-2.41439075  0.51628447]\n",
      " [-2.62966145  0.40774632]\n",
      " [-2.53931232  0.53331485]\n",
      " [-2.52016653 -0.07628126]]\n",
      "First five lines of the Transformed data using the StandardScaler:\n",
      " [[-2.26454173 -0.5057039 ]\n",
      " [-2.0864255   0.65540473]\n",
      " [-2.36795045  0.31847731]\n",
      " [-2.30419716  0.57536771]\n",
      " [-2.38877749 -0.6747674 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained variance:\", pca_norm.explained_variance)\n",
    "print(\"Explained variance using the StandardScaler:\", pca_scaled.explained_variance)\n",
    "\n",
    "print(\"\\nTransformed data structure (normalized):\", X_transformed_norm.shape)\n",
    "print(\"Transformed data structure using the StandardScaler:\", X_transformed_scaled.shape)\n",
    "\n",
    "print (\"\\nFirst five lines of the Transformed data (normalized):\\n\", X_transformed_norm[:5])\n",
    "print(\"First five lines of the Transformed data using the StandardScaler:\\n\", X_transformed_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 6:** Implementing stratified splitting\n",
    "\n",
    "Test the \"stratified_train_test_split\" function with the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.model_selection.split import train_test_split, stratified_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(iris_dataset)\n",
    "train_strat, test_strat = stratified_train_test_split(iris_dataset)\n",
    "train_strat_50, test_strat_50 = stratified_train_test_split(iris_dataset, test_size=0.5)\n",
    "train_strat_seed, test_strat_seed= stratified_train_test_split(iris_dataset, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test dataset sizes: (120, 4) (30, 4)\n",
      "Train and Test dataset sizes (stratified): (120, 4) (30, 4)\n",
      "Train and Test dataset sizes (stratified), with half used for training: (75, 4) (75, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and test dataset sizes:\", train.X.shape, test.X.shape)\n",
    "print(\"Train and Test dataset sizes (stratified):\", train_strat.X.shape, test_strat.X.shape)\n",
    "print(\"Train and Test dataset sizes (stratified), with half used for training:\", train_strat_50.X.shape, test_strat_50.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size, with default seed:\n",
      " [[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]]\n",
      "Test dataset size, with default seed:\n",
      " [[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]]\n",
      "\n",
      "Train dataset size (stratified), with default seed:\n",
      " [[4.8 3.  1.4 0.1]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n",
      "Test dataset size (stratified), with default seed:\n",
      " [[4.3 3.  1.1 0.1]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.5 1.4 0.3]]\n",
      "\n",
      "Train adataset size (stratified), with seed=5:\n",
      " [[5.4 3.4 1.5 0.4]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.3 1.7 0.5]]\n",
      "Test dataset size (stratified), with seed=5:\n",
      " [[4.4 3.2 1.3 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.2 3.4 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size, with default seed:\\n\", train.X[:5])\n",
    "print(\"Test dataset size, with default seed:\\n\", test.X[:5])\n",
    "\n",
    "print(\"\\nTrain dataset size (stratified), with default seed:\\n\", train_strat.X[:5])\n",
    "print(\"Test dataset size (stratified), with default seed:\\n\", test_strat.X[:5])\n",
    "\n",
    "print(\"\\nTrain adataset size (stratified), with seed=5:\\n\", train_strat_seed.X[:5])\n",
    "print(\"Test dataset size (stratified), with seed=5:\\n\", test_strat_seed.X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 7:** mplementing the KNNRegressor with RMSE\n",
    "\n",
    "7.3. Test the \"KNNRegressor\" class using the \"cpu.csv\" dataset (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.metrics.rmse import rmse\n",
    "from si.models.knn_regressor import KNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 198,  269,  220,  172,  132,  318,  367,  489,  636, 1144,   38,\n",
       "         40,   92,  138,   10,   35,   19,   28,   31,  120,   30,   33,\n",
       "         61,   76,   23,   69,   33,   27,   77,   27,  274,  368,   32,\n",
       "         63,  106,  208,   20,   29,   71,   26,   36,   40,   52,   60,\n",
       "         72,   72,   18,   20,   40,   62,   24,   24,  138,   36,   26,\n",
       "         60,   71,   12,   14,   20,   16,   22,   36,  144,  144,  259,\n",
       "         17,   26,   32,   32,   62,   64,   22,   36,   44,   50,   45,\n",
       "         53,   36,   84,   16,   38,   38,   16,   22,   29,   40,   35,\n",
       "        134,   66,  141,  189,   22,  132,  237,  465,  465,  277,  185,\n",
       "          6,   24,   45,    7,   13,   16,   32,   32,   11,   11,   18,\n",
       "         22,   37,   40,   34,   50,   76,   66,   24,   49,   66,  100,\n",
       "        133,   12,   18,   20,   27,   45,   56,   70,   80,  136,   16,\n",
       "         26,   32,   45,   54,   65,   30,   50,   40,   62,   60,   50,\n",
       "         66,   86,   74,   93,  110,  143,  105,  214,  277,  370,  510,\n",
       "        214,  326,  510,    8,   12,   17,   21,   24,   34,   42,   46,\n",
       "         51,  116,  100,  140,  212,   25,   30,   41,   25,   50,   50,\n",
       "         30,   32,   38,   60,  109,    6,   11,   22,   33,   58,  130,\n",
       "         75,  113,  188,  173,  248,  405,   70,  114,  208,  307,  397,\n",
       "        915, 1150,   12,   14,   18,   21,   42,   46,   52,   67,   45])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu = read_csv('../datasets/cpu/cpu.csv', features=True, label=True)\n",
    "\n",
    "cpu.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test dataset sizes: (192, 6) (17, 6)\n",
      "Train dataset:\n",
      " [[ 240  512 1000    8    1    3]\n",
      " [ 480   96  512    0    1    1]\n",
      " [1100  512 1500    0    1    1]\n",
      " [ 112 1000 1000    0    1    4]\n",
      " [ 350   64   64    0    1    4]]\n",
      "Test dataset:\n",
      " [[1500  768 1000    0    0    0]\n",
      " [ 100 1000 8000    0    2    6]\n",
      " [1500  768 2000    0    0    0]\n",
      " [  50  500 2000    8    1    4]\n",
      " [ 800  256 8000    0    1    4]]\n"
     ]
    }
   ],
   "source": [
    "train_cpu, test_cpu = stratified_train_test_split(cpu, test_size=0.3)\n",
    "print(\"Train and test dataset sizes:\", train_cpu.X.shape, test_cpu.X.shape)\n",
    "print(\"Train dataset:\\n\", train_cpu.X[:5])\n",
    "print(\"Test dataset:\\n\", test_cpu.X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.,  26.,  13.,  32.,  14.,  25.,  22.,  33.,  63.,  26.,  44.,\n",
       "        50.,  62.,  22., 109.,  72.,  70.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn regressor with k=1 + stratified split\n",
    "kmeans = KNNRegressor()\n",
    "kmeans.fit(train_cpu)\n",
    "predictions = kmeans.predict(test_cpu)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.126513780921098"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(test_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12. , 43.6, 16. , 30.6, 17.2, 20. , 30.6, 38.6, 73.2, 27.2, 33. ,\n",
       "       73. , 52. , 38.2, 64.4, 41.6, 67.8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn regressor with k=5 + stratified split\n",
    "kmeans = KNNRegressor(k=5)\n",
    "kmeans.fit(train_cpu)\n",
    "predictions = kmeans.predict(test_cpu)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.638297424209924"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(test_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing RidgeRegression**\n",
    "\n",
    "1. Use thedataset cpu.csv\n",
    "2. Divide the dataset into train and test sets\n",
    "3. Train the model. Which score do you get? And the cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 9971.186660728232\n",
      "Cost: 5041.381040697732\n"
     ]
    }
   ],
   "source": [
    "from si.models.linear_regression import RidgeRegression\n",
    "\n",
    "cpu_dataset = read_csv('../datasets/cpu/cpu.csv', features=True, label=True)\n",
    "\n",
    "cpu_train, cpu_test = train_test_split(cpu_dataset, test_size=0.2)\n",
    "\n",
    "ridge_model = RidgeRegression()\n",
    "ridge_model.fit(cpu_train)\n",
    "predictions = ridge_model.predict(cpu_test)\n",
    "test_score = ridge_model.score(cpu_test)\n",
    "test_cost = ridge_model.cost(cpu_test)\n",
    "\n",
    "print(\"Score:\", test_score)\n",
    "print(\"Cost:\", test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2688.1340706281962\n",
      "Cost: 1750.9645571409383\n"
     ]
    }
   ],
   "source": [
    "cpu_train_strat, cpu_test_strat = stratified_train_test_split(cpu_dataset, test_size=0.2)\n",
    "\n",
    "ridge_model2 = RidgeRegression()\n",
    "ridge_model2.fit(cpu_train_strat)\n",
    "predictions_strat = ridge_model2.predict(cpu_test_strat)\n",
    "test_score_strat = ridge_model2.score(cpu_test_strat)\n",
    "test_cost_strat = ridge_model2.cost(cpu_test_strat)\n",
    "\n",
    "print(\"Score:\", test_score_strat)\n",
    "print(\"Cost:\", test_cost_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 836.1541938117806\n",
      "Cost: 1058.334640118596\n"
     ]
    }
   ],
   "source": [
    "ridge_model2 = RidgeRegression(alpha=0.1)\n",
    "ridge_model2.fit(cpu_train_strat)\n",
    "predictions_strat = ridge_model2.predict(cpu_test_strat)\n",
    "test_score_strat = ridge_model2.score(cpu_test_strat)\n",
    "test_cost_strat = ridge_model2.cost(cpu_test_strat)\n",
    "\n",
    "print(\"Score:\", test_score_strat)\n",
    "print(\"Cost:\", test_cost_strat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
