{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: NumPy array Indexing/Slicing\n",
    "\n",
    "**Ex 1.1:** Load the \"iris.csv\" using the appropriate method for this file type (use the new functions from the package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_df = pd.read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\iris\\iris.csv')\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.2:** Select the penultimate independent variable. What is the dimension of the resulting array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the resulting array: (150,)\n"
     ]
    }
   ],
   "source": [
    "penultimate_variable = iris_df.iloc[:, -2]\n",
    "print(\"Dimension of the resulting array:\", penultimate_variable.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.3:** Select the last 10 samples from the iris dataset. What is the mean of the last 10 samples for each\n",
    "independent variable/feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the last 10 samples for each independent variable/feature:\n",
      " sepal_length    6.45\n",
      "sepal_width     3.03\n",
      "petal_length    5.33\n",
      "petal_width     2.17\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "last10 = iris_df.iloc[-10:, :-1]\n",
    "mean_last10 = last10.mean()\n",
    "print(\"Mean of the last 10 samples for each independent variable/feature:\\n\", mean_last10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.4:** Select all samples from the dataset with values less than or equal to 6 for all independent variables/features. How many samples do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with values less than or equal to 6 for all independent variables/features: 89\n"
     ]
    }
   ],
   "source": [
    "filtered_samples = iris_df[(iris_df.iloc[:, :-1] <= 6).all(axis=1)]\n",
    "num_samples = filtered_samples.shape[0]\n",
    "print(\"Number of samples with values less than or equal to 6 for all independent variables/features:\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex 1.5:** Select all samples with a class/label different from 'Iris-setosa'. How many samples do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with a class/label different from 'Iris-setosa': 100\n"
     ]
    }
   ],
   "source": [
    "samples = iris_df[iris_df['class'] != 'Iris-setosa']\n",
    "num_samples = samples = iris_df[iris_df['class'] != 'Iris-setosa'].shape[0]\n",
    "print(\"Number of samples with a class/label different from 'Iris-setosa':\", num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2:** \n",
    "\n",
    "Examples of how to use the fillna, dropna and remove_by_index methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from si.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "X: [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [nan 3.  5.6 nan]]\n",
      "y: ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "#Turning the iris dataset into a Dataset object and adding a row with NaN values\n",
    "X = last10.values\n",
    "new_row = np.array([np.nan, 3., 5.6, np.nan])\n",
    "X = np.vstack([X, new_row])\n",
    "y = iris_df.iloc[-10:, -1].values\n",
    "new_y = np.array(['Iris-setosa'])\n",
    "y = np.append(y, new_y)\n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "print(\"Original Dataset:\")\n",
    "print(\"X:\", dataset.X)\n",
    "print(\"y:\", dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains missing values.\n"
     ]
    }
   ],
   "source": [
    "if np.isnan(dataset.X).any() :\n",
    "    print(\"The dataset contains missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copy: \n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [nan 3.  5.6 nan]]\n",
      "['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n",
      "\n",
      " X after dropna:\n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y after dropna: ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "#Making a copy of the dataset\n",
    "X_copy = np.copy(dataset.X)\n",
    "y_copy = np.copy(dataset.y) \n",
    "dataset_copy = Dataset(X_copy, y_copy)\n",
    "\n",
    "print(\"Dataset copy: \\n\", dataset_copy.X)\n",
    "print(dataset_copy.y)\n",
    "\n",
    "# Removing samples with missing values using the dropna method\n",
    "cleaned_dataset = dataset_copy.dropna()\n",
    "print(\"\\n X after dropna:\\n\", cleaned_dataset.X)\n",
    "print(\"y after dropna:\", cleaned_dataset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X after fillna:\n",
      " [[ 6.7  3.1  5.6  2.4]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]\n",
      " [10.   3.   5.6 10. ]]\n",
      "y after fillna:\n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values with the mean of the corresponding feature using the fillna method\n",
    "filled_dataset = dataset.fillna(10.0)\n",
    "print(\"X after fillna:\\n\", filled_dataset.X)\n",
    "print(\"y after fillna:\\n\", filled_dataset.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the modified dataset: (10, 4)\n",
      "X after remove_by_index:\n",
      " [[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y after remove_by_index:\n",
      " ['Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# Removing the last sample using the remove_by_index method\n",
    "removed_dataset = filled_dataset.remove_by_index(-1)\n",
    "print(\"Size of the modified dataset:\", dataset.X.shape)\n",
    "print(\"X after remove_by_index:\\n\", removed_dataset.X)\n",
    "print(\"y after remove_by_index:\\n\", removed_dataset.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 3:** Implementing SelectPercentile\n",
    "\n",
    "Testing the SelectPercentile class using the \"iris.csv\" dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.io.csv_file import read_csv\n",
    "from si.statistics.f_classification import f_classification\n",
    "from si.feature_selection.select_percentile import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (150, 4)\n",
      "Transformed dataset shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)\n",
    "\n",
    "# Create an instance of the SelectPercentile class\n",
    "selector = SelectPercentile(percentile=50)\n",
    "# Fit the selector to the dataset\n",
    "selector.fit(iris_dataset)\n",
    "\n",
    "# Transform the dataset using the fitted model\n",
    "transformed_dataset = selector.transform(iris_dataset)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original dataset shape:\", iris_dataset.X.shape)\n",
    "print(\"Transformed dataset shape:\", transformed_dataset.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F values: [ 119.26450218   47.3644614  1179.0343277   959.32440573]\n",
      "p values: [1.66966919e-31 1.32791652e-16 3.05197580e-91 4.37695696e-85]\n",
      "Original dataset shape: (150, 4)\n",
      "Transformed dataset shape: (150, 2)\n",
      "Original features:\n",
      " Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], dtype='object')\n",
      "Selected features:\n",
      " ['petal_width', 'petal_length']\n"
     ]
    }
   ],
   "source": [
    "selector2 = SelectPercentile()\n",
    "selector2.fit(iris_dataset)\n",
    "transformed_dataset2 = selector.transform(iris_dataset)\n",
    "\n",
    "print(\"F values:\", selector2.F)\n",
    "print(\"p values:\", selector2.p)\n",
    "\n",
    "print(\"Original dataset shape:\", iris_dataset.X.shape)\n",
    "print(\"Transformed dataset shape:\", transformed_dataset2.X.shape)\n",
    "print(\"Original features:\\n\", iris_dataset.features)\n",
    "print(\"Selected features:\\n\", transformed_dataset2.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercício 5:** PCA\n",
    "\n",
    "Testing the PCA class in a jupyter notebook using the iris.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.decomposition.pca import PCA\n",
    "from sklearn.decomposition import PCA as PCA_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class\n",
    "np.random.seed(5)\n",
    "pca = PCA(n_components=2)\n",
    "pca._fit(iris_dataset.X)\n",
    "X_transformed = pca._transform(iris_dataset.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class from scikit-learn\n",
    "np.random.seed(5)\n",
    "pca_sklearn = PCA_sklearn(n_components=2)\n",
    "pca_sklearn.fit(iris_dataset.X)\n",
    "X_transformed_sklearn = pca_sklearn.transform(iris_dataset.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [0.92461621 0.05301557]\n",
      "Explained variance by PCA from scikit-learn: [4.22484077 0.24224357]\n",
      "\n",
      "Transformed data structure: (150, 2)\n",
      "Transformed data structure by PCA from scikit-learn: (150, 2)\n",
      "\n",
      "First five lines of the Transformed data:\n",
      " [[-2.68420713 -0.32660731]\n",
      " [-2.71539062  0.16955685]\n",
      " [-2.88981954  0.13734561]\n",
      " [-2.7464372   0.31112432]\n",
      " [-2.72859298 -0.33392456]]\n",
      "First five lines of the Transformed data by PCA from scikit-learn:\n",
      " [[-2.68420713  0.32660731]\n",
      " [-2.71539062 -0.16955685]\n",
      " [-2.88981954 -0.13734561]\n",
      " [-2.7464372  -0.31112432]\n",
      " [-2.72859298  0.33392456]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained variance:\", pca.explained_variance)\n",
    "print(\"Explained variance by PCA from scikit-learn:\", pca_sklearn.explained_variance_)\n",
    "\n",
    "print(\"\\nTransformed data structure:\", X_transformed.shape)\n",
    "print(\"Transformed data structure by PCA from scikit-learn:\", X_transformed_sklearn.shape)\n",
    "\n",
    "print (\"\\nFirst five lines of the Transformed data:\\n\", X_transformed[:5])\n",
    "print(\"First five lines of the Transformed data by PCA from scikit-learn:\\n\", X_transformed_sklearn[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class and normalizing the data\n",
    "pca_norm = PCA(n_components=2)\n",
    "pca_norm._fit(iris_dataset.X, normalization=True)\n",
    "X_transformed_norm = pca_norm._transform(iris_dataset.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components using PCA class and normalizing the data using the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "np.random.seed(5)\n",
    "X_scaled = StandardScaler().fit_transform(iris_dataset.X)\n",
    "pca_scaled = PCA(n_components=2)\n",
    "pca_scaled._fit(X_scaled)\n",
    "X_transformed_scaled = pca_scaled._transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [0.72770452 0.23030523]\n",
      "Explained variance using the StandardScaler: [0.72770452 0.23030523]\n",
      "\n",
      "Transformed data structure (normalized): (150, 2)\n",
      "Transformed data structure using the StandardScaler: (150, 2)\n",
      "\n",
      "First five lines of the Transformed data (normalized):\n",
      " [[-2.44159388 -0.02095745]\n",
      " [-2.41439075  0.51628447]\n",
      " [-2.62966145  0.40774632]\n",
      " [-2.53931232  0.53331485]\n",
      " [-2.52016653 -0.07628126]]\n",
      "First five lines of the Transformed data using the StandardScaler:\n",
      " [[-2.26454173 -0.5057039 ]\n",
      " [-2.0864255   0.65540473]\n",
      " [-2.36795045  0.31847731]\n",
      " [-2.30419716  0.57536771]\n",
      " [-2.38877749 -0.6747674 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained variance:\", pca_norm.explained_variance)\n",
    "print(\"Explained variance using the StandardScaler:\", pca_scaled.explained_variance)\n",
    "\n",
    "print(\"\\nTransformed data structure (normalized):\", X_transformed_norm.shape)\n",
    "print(\"Transformed data structure using the StandardScaler:\", X_transformed_scaled.shape)\n",
    "\n",
    "print (\"\\nFirst five lines of the Transformed data (normalized):\\n\", X_transformed_norm[:5])\n",
    "print(\"First five lines of the Transformed data using the StandardScaler:\\n\", X_transformed_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 6:** Implementing stratified splitting\n",
    "\n",
    "Test the \"stratified_train_test_split\" function with the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.model_selection.split import train_test_split, stratified_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(iris_dataset)\n",
    "train_strat, test_strat = stratified_train_test_split(iris_dataset)\n",
    "train_strat_50, test_strat_50 = stratified_train_test_split(iris_dataset, test_size=0.5)\n",
    "train_strat_seed, test_strat_seed= stratified_train_test_split(iris_dataset, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test dataset sizes: (120, 4) (30, 4)\n",
      "Train and Test dataset sizes (stratified): (120, 4) (30, 4)\n",
      "Train and Test dataset sizes (stratified), with half used for training: (75, 4) (75, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and test dataset sizes:\", train.X.shape, test.X.shape)\n",
    "print(\"Train and Test dataset sizes (stratified):\", train_strat.X.shape, test_strat.X.shape)\n",
    "print(\"Train and Test dataset sizes (stratified), with half used for training:\", train_strat_50.X.shape, test_strat_50.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size, with default seed:\n",
      " [[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]]\n",
      "Test dataset size, with default seed:\n",
      " [[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]]\n",
      "\n",
      "Train dataset size (stratified), with default seed:\n",
      " [[4.8 3.  1.4 0.1]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n",
      "Test dataset size (stratified), with default seed:\n",
      " [[4.3 3.  1.1 0.1]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.5 1.4 0.3]]\n",
      "\n",
      "Train adataset size (stratified), with seed=5:\n",
      " [[5.4 3.4 1.5 0.4]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.3 1.7 0.5]]\n",
      "Test dataset size (stratified), with seed=5:\n",
      " [[4.4 3.2 1.3 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.2 3.4 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size, with default seed:\\n\", train.X[:5])\n",
    "print(\"Test dataset size, with default seed:\\n\", test.X[:5])\n",
    "\n",
    "print(\"\\nTrain dataset size (stratified), with default seed:\\n\", train_strat.X[:5])\n",
    "print(\"Test dataset size (stratified), with default seed:\\n\", test_strat.X[:5])\n",
    "\n",
    "print(\"\\nTrain adataset size (stratified), with seed=5:\\n\", train_strat_seed.X[:5])\n",
    "print(\"Test dataset size (stratified), with seed=5:\\n\", test_strat_seed.X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 7:** mplementing the KNNRegressor with RMSE\n",
    "\n",
    "7.3. Test the \"KNNRegressor\" class using the \"cpu.csv\" dataset (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.metrics.rmse import rmse\n",
    "from si.models.knn_regressor import KNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 198,  269,  220,  172,  132,  318,  367,  489,  636, 1144,   38,\n",
       "         40,   92,  138,   10,   35,   19,   28,   31,  120,   30,   33,\n",
       "         61,   76,   23,   69,   33,   27,   77,   27,  274,  368,   32,\n",
       "         63,  106,  208,   20,   29,   71,   26,   36,   40,   52,   60,\n",
       "         72,   72,   18,   20,   40,   62,   24,   24,  138,   36,   26,\n",
       "         60,   71,   12,   14,   20,   16,   22,   36,  144,  144,  259,\n",
       "         17,   26,   32,   32,   62,   64,   22,   36,   44,   50,   45,\n",
       "         53,   36,   84,   16,   38,   38,   16,   22,   29,   40,   35,\n",
       "        134,   66,  141,  189,   22,  132,  237,  465,  465,  277,  185,\n",
       "          6,   24,   45,    7,   13,   16,   32,   32,   11,   11,   18,\n",
       "         22,   37,   40,   34,   50,   76,   66,   24,   49,   66,  100,\n",
       "        133,   12,   18,   20,   27,   45,   56,   70,   80,  136,   16,\n",
       "         26,   32,   45,   54,   65,   30,   50,   40,   62,   60,   50,\n",
       "         66,   86,   74,   93,  110,  143,  105,  214,  277,  370,  510,\n",
       "        214,  326,  510,    8,   12,   17,   21,   24,   34,   42,   46,\n",
       "         51,  116,  100,  140,  212,   25,   30,   41,   25,   50,   50,\n",
       "         30,   32,   38,   60,  109,    6,   11,   22,   33,   58,  130,\n",
       "         75,  113,  188,  173,  248,  405,   70,  114,  208,  307,  397,\n",
       "        915, 1150,   12,   14,   18,   21,   42,   46,   52,   67,   45])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu = read_csv('../datasets/cpu/cpu.csv', features=True, label=True)\n",
    "\n",
    "cpu.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test dataset sizes: (192, 6) (17, 6)\n",
      "Train dataset:\n",
      " [[ 480   96  512    0    1    1]\n",
      " [ 240  512 1000    8    1    3]\n",
      " [1100  512 1500    0    1    1]\n",
      " [ 112 1000 1000    0    1    4]\n",
      " [ 350   64   64    0    1    4]]\n",
      "Test dataset:\n",
      " [[ 180  262 4000    0    1    3]\n",
      " [ 330 1000 2000    0    1    2]\n",
      " [ 900 1000 4000    4    1    2]\n",
      " [ 800  256 8000    0    1    4]\n",
      " [ 330 1000 4000    0    3    6]]\n"
     ]
    }
   ],
   "source": [
    "train_cpu, test_cpu = stratified_train_test_split(cpu, test_size=0.3, random_state=5)\n",
    "print(\"Train and test dataset sizes:\", train_cpu.X.shape, test_cpu.X.shape)\n",
    "print(\"Train dataset:\\n\", train_cpu.X[:5])\n",
    "print(\"Test dataset:\\n\", test_cpu.X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18., 24., 22., 14., 38., 56., 32., 24., 34., 34., 44., 16., 25.,\n",
       "       52., 25., 52., 70.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn regressor with k=1 + stratified split\n",
    "kmeans = KNNRegressor()\n",
    "kmeans.fit(train_cpu)\n",
    "predictions = kmeans.predict(test_cpu)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.37029191462748"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(test_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21.6,  21.6,  33.4,  17.6,  34.2,  67.8,  41.6,  31.6,  33. ,\n",
       "        61.2, 145.8, 145.8,  23.4,  56.2,  24. ,  59.2,  67.8])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn regressor with k=5 + stratified split\n",
    "kmeans = KNNRegressor(k=5)\n",
    "kmeans.fit(train_cpu)\n",
    "predictions = kmeans.predict(test_cpu)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.79434793607335"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.score(test_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing RidgeRegression**\n",
    "\n",
    "1. Use thedataset cpu.csv\n",
    "2. Divide the dataset into train and test sets\n",
    "3. Train the model. Which score do you get? And the cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 12721.613689859541\n",
      "Cost: 6429.521624869436\n"
     ]
    }
   ],
   "source": [
    "from si.models.linear_regression import RidgeRegression\n",
    "\n",
    "cpu_dataset = read_csv('../datasets/cpu/cpu.csv', features=True, label=True)\n",
    "\n",
    "cpu_train, cpu_test = train_test_split(cpu_dataset, test_size=0.2, random_state=5)\n",
    "\n",
    "ridge_model = RidgeRegression()\n",
    "ridge_model.fit(cpu_train)\n",
    "predictions = ridge_model.predict(cpu_test)\n",
    "test_score = ridge_model.score(cpu_test)\n",
    "test_cost = ridge_model.cost(cpu_test)\n",
    "\n",
    "print(\"Score:\", test_score)\n",
    "print(\"Cost:\", test_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2962.5932159158297\n",
      "Cost: 1889.033227942686\n"
     ]
    }
   ],
   "source": [
    "cpu_train_strat, cpu_test_strat = stratified_train_test_split(cpu_dataset, test_size=0.2, random_state=5)\n",
    "\n",
    "ridge_model2 = RidgeRegression()\n",
    "ridge_model2.fit(cpu_train_strat)\n",
    "predictions_strat = ridge_model2.predict(cpu_test_strat)\n",
    "test_score_strat = ridge_model2.score(cpu_test_strat)\n",
    "test_cost_strat = ridge_model2.cost(cpu_test_strat)\n",
    "\n",
    "print(\"Score:\", test_score_strat)\n",
    "print(\"Cost:\", test_cost_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1107.5728118060001\n",
      "Cost: 1199.0057277113979\n"
     ]
    }
   ],
   "source": [
    "ridge_model2 = RidgeRegression(alpha=0.1)\n",
    "ridge_model2.fit(cpu_train_strat)\n",
    "predictions_strat = ridge_model2.predict(cpu_test_strat)\n",
    "test_score_strat = ridge_model2.score(cpu_test_strat)\n",
    "test_cost_strat = ridge_model2.cost(cpu_test_strat)\n",
    "\n",
    "print(\"Score:\", test_score_strat)\n",
    "print(\"Cost:\", test_cost_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TestingRidgeRegression and LogisticRegression** using the breast-bin.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.models.logistic_regression import LogisticRegression\n",
    "\n",
    "bb_dataset = read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\breast_bin\\breast-bin.csv', features=True, label=True)\n",
    "\n",
    "bb_train, bb_test = stratified_train_test_split(bb_dataset, test_size=0.3, random_state=5)\n",
    "\n",
    "# Ridge regression model\n",
    "ridge_model = RidgeRegression(alpha=0.1)\n",
    "ridge_model.fit(bb_train)\n",
    "predictions_R = ridge_model.predict(bb_test)\n",
    "test_score_R = ridge_model.score(bb_test)\n",
    "test_cost_R = ridge_model.cost(bb_test)\n",
    "\n",
    "# Logistic regression model\n",
    "logistic_model = LogisticRegression(alpha=0.1)\n",
    "logistic_model.fit(bb_train)\n",
    "predictions_L = logistic_model.predict(bb_test)\n",
    "test_score_L = logistic_model.score(bb_test)\n",
    "test_cost_L = logistic_model.cost(bb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for the Ridge Regression: 0.03588477223004671\n",
      "Cost for the Ridge Regression: 0.018055436565692178\n",
      "\n",
      "Score for the Logistic Regression: 0.9760765550239234\n",
      "Cost for the Logistic Regression: 1.445038601820536\n"
     ]
    }
   ],
   "source": [
    "print(\"Score for the Ridge Regression:\", test_score_R)\n",
    "print(\"Cost for the Ridge Regression:\", test_cost_R)\n",
    "\n",
    "print(\"\\nScore for the Logistic Regression:\", test_score_L)\n",
    "print(\"Cost for the Logistic Regression:\", test_cost_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 9.2:** Testing Random Forest\n",
    "1.Use the iris.csv dataset\n",
    "\n",
    "2.Split the data into train and test sets\n",
    "\n",
    "3.Create the RandomForestClassifier model\n",
    "\n",
    "4.Train the model. What is the score of the model on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.models.random_forest_classifier import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<si.models.random_forest_classifier.RandomForestClassifier at 0x2246d9ef560>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset = read_csv('../datasets/iris/iris.csv', features=True, label=True)\n",
    "train_iris, test_iris = stratified_train_test_split(iris_dataset, test_size=0.2, random_state=5)\n",
    "\n",
    "rf_model = RandomForestClassifier(seed=5)\n",
    "rf_model.fit(train_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "predictions_rf = rf_model.predict(test_iris)\n",
    "test_score_rf = rf_model.score(test_iris)\n",
    "\n",
    "print(\"Score:\", test_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 10.2:** Testing the StackingClassifier model\n",
    "\n",
    "1.Use the breast-bin.csv dataset\n",
    "\n",
    "2.Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10 = read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\breast_bin\\breast-bin.csv', features=True, label=True)\n",
    "\n",
    "train_10, test_10 = stratified_train_test_split(dataset_10, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Create a KNNClassifier model\n",
    "\n",
    "4.Create a LogisticRegression model\n",
    "\n",
    "5.Create a DecisionTree model\n",
    "\n",
    "6.Create a second KNNClassifier model (final model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.models.knn_classifier import KNNClassifier\n",
    "from si.models.decision_tree_classifier import DecisionTreeClassifier\n",
    "\n",
    "knn_model = KNNClassifier()\n",
    "log_reg_model = LogisticRegression()\n",
    "decision_tree_model = DecisionTreeClassifier(max_depth=5)\n",
    "final_model = KNNClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Create a StackingClassifier model using the previous classifiers. The second KNNClassifier model must be used as the final model.\n",
    "\n",
    "8.Train the StackingClassifier model. What is the score of the model on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9827586206896551\n"
     ]
    }
   ],
   "source": [
    "from si.ensemble.stacking_classifier import StackingClassifier\n",
    "\n",
    "stacking_model = StackingClassifier(models=[knn_model, log_reg_model, decision_tree_model], final_model=final_model)\n",
    "stacking_model.fit(train_10)\n",
    "test_score_stacking = stacking_model.score(test_10)\n",
    "\n",
    "print(\"Score:\", test_score_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 11:** Implementing the randomized_search_cv function\n",
    "1. Use the breast-bin.csv dataset\n",
    "\n",
    "2. Create a LogisticRegression model\n",
    "\n",
    "3. Perform a randomized search with the following hyperparameter distributions:\n",
    "* l2_penalty: distribution between 1 and 10 with 10 equal intervals (e.g., np.linspace(1, 10, 10))\n",
    "* alpha: distribution between 0.001 and 0.0001 with 100 equal intervals (e.g., np.linspace(0.001, 0.0001, 100))\n",
    "* max_iter: distribution between 1000 and 2000 with 200 equal intervals (e.g., np.linspace(1000, 2000, 200))\n",
    "\n",
    "4. Use n_iter=10 and cv=3 folds for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[5. 1. 1. 3. 2. 1. 1. 1. 1.]\n",
      " [2. 1. 1. 1. 3. 1. 2. 1. 1.]\n",
      " [2. 1. 1. 1. 2. 1. 3. 1. 1.]\n",
      " [9. 5. 8. 1. 2. 3. 2. 1. 5.]\n",
      " [3. 1. 1. 1. 2. 1. 3. 1. 1.]]\n",
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "\n",
    "breast_bin = read_csv(r'C:\\Users\\35191\\Documents\\GitHub\\si\\datasets\\breast_bin\\breast-bin.csv', features=True, label=True)\n",
    "\n",
    "print(hasattr(breast_bin, 'X'))  # Should output: True\n",
    "print(hasattr(breast_bin, 'y'))  # Should output: True\n",
    "print(type(breast_bin.X))  # Should output: <class 'numpy.ndarray'>\n",
    "print(type(breast_bin.y))  # Should output: <class 'numpy.ndarray'>\n",
    "breast_bin.X.shape  # Should output: (699, 9)\n",
    "print(breast_bin.X[:5])  # Preview the first 5 rows of the feature matrix\n",
    "print(breast_bin.y[:5])  # Preview the first 5 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m model_log \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m      7\u001b[0m hyperparameter_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m) ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m100\u001b[39m), \n\u001b[0;32m      8\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m200\u001b[39m) }\n\u001b[1;32m---> 10\u001b[0m dict_results \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_search_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbreast_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mhyperparameter_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\35191\\documents\\github\\si\\src\\si\\model_selection\\randomized_search.py:44\u001b[0m, in \u001b[0;36mrandomized_search_cv\u001b[1;34m(model, dataset, hyperparameter_grid, scoring, cv, n_iter)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(model, parameter, value)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Cross validate the model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(score)) \u001b[38;5;66;03m# Save the mean of the scores\u001b[39;00m\n\u001b[0;32m     46\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(combination) \u001b[38;5;66;03m#Save the hyperparameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\35191\\documents\\github\\si\\src\\si\\model_selection\\cross_validate.py:55\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[1;34m(model, dataset, scoring, cv, seed)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Fit the model on the training set and score it on the test set\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(dataset_train)\n\u001b[1;32m---> 55\u001b[0m     fold_score \u001b[38;5;241m=\u001b[39m \u001b[43mscoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m scoring \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mscore(\n\u001b[0;32m     56\u001b[0m         dataset_test)\n\u001b[0;32m     57\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(fold_score)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\users\\35191\\documents\\github\\si\\src\\si\\model_selection\\cross_validate.py:31\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[1;34m(model, dataset, scoring, cv, seed)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mk_fold_cross_validation\u001b[39m(model, dataset: Dataset, scoring: \u001b[38;5;28mcallable\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, cv: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      9\u001b[0m                             seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Perform k-fold cross-validation on the given model and dataset.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m        The scores of the model on each fold.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m     fold_size \u001b[38;5;241m=\u001b[39m num_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m cv\n\u001b[0;32m     33\u001b[0m     scores \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'X'"
     ]
    }
   ],
   "source": [
    "from si.models.logistic_regression import LogisticRegression\n",
    "from si.model_selection.randomized_search import randomized_search_cv\n",
    "import numpy as np\n",
    "\n",
    "model_log = LogisticRegression()\n",
    "\n",
    "hyperparameter_grid = {'l2_penalty': np.linspace(1, 10, 10) ,'alpha': np.linspace(0.001, 0.0001, 100), \n",
    "                       'max_iter': np.linspace(1000, 2000, 200) }\n",
    "\n",
    "dict_results = randomized_search_cv(model=model_log, \n",
    "                                    dataset=breast_bin, \n",
    "                                    hyperparameter_grid=hyperparameter_grid, \n",
    "                                    n_iter=10, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which scores do you obtain? What are the best score and best hyperparameters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
