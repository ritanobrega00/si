{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook for the testing of Neuronal Networks exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 12:** Testing the Dropout layer\n",
    "\n",
    "12.2. Test the layer with a random input and check if the output shows the desired behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during training:\n",
      "[[1.08752735 0.         0.         0.44951561 0.         1.3261253\n",
      "  0.         0.         0.         0.        ]\n",
      " [1.44273565 0.         1.63477972 0.07118287 1.89823008 0.\n",
      "  0.         0.         0.         0.68037116]\n",
      " [1.59787978 0.         1.48663138 0.         0.36319677 0.\n",
      "  0.91840551 1.43387143 0.         0.        ]\n",
      " [1.28486646 0.490595   0.         0.         0.         0.42708262\n",
      "  1.78982273 0.17401187 0.         0.        ]\n",
      " [0.         0.         0.04019392 0.         1.09263108 0.\n",
      "  0.         0.24729515 0.62120223 1.08551303]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.layers import Dropout\n",
    "\n",
    "dropout_layer = Dropout(probability=0.5)\n",
    "input_data = np.random.rand(5, 10)\n",
    "dropout_layer.set_input_shape(input_data.shape)  \n",
    "\n",
    "# Forward propagation during training --> expected to have some zeros due to the mask\n",
    "output_train = dropout_layer.forward_propagation(input_data, training=True)\n",
    "print(\"Output during training:\")\n",
    "print(output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output during inference is the same as the input.\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation during inference --> shoulb the same as the input\n",
    "output_inference = dropout_layer.forward_propagation(input_data, training=False)\n",
    "if (output_inference == input_data).all():\n",
    "    print(\"Output during inference is the same as the input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input error during backpropagation:\n",
      "[[1. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 1. 0. 1. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Backward propagation --> by using a dummy error for testing, we should see some zeros in the input error\n",
    "error = np.ones_like(input_data) \n",
    "input_error = dropout_layer.backward_propagation(error)\n",
    "print(\"Input error during backpropagation:\")\n",
    "print(input_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape is correct\n"
     ]
    }
   ],
   "source": [
    "# Output shape that shpuld be the same as the input shape\n",
    "if dropout_layer.output_shape() == input_data.shape:\n",
    "    print(\"Output shape is correct\")\n",
    "else:\n",
    "    print(\"Something is wrong: Output shape is incorrect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# Parameters of the dropout layer - should be 0\n",
    "if dropout_layer.parameters() == 0:\n",
    "    print(\"Parameters: 0\")\n",
    "else:\n",
    "    print(\"Something is wrong: dropout layers do not have learnable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 13:** TanhActivation and SoftmaxActivation classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tanh Activation Output:\n",
      " [[ 0.76159416  0.96402758  0.99505475]\n",
      " [-0.76159416 -0.96402758 -0.99505475]\n",
      " [ 0.46211716  0.         -0.46211716]]\n",
      "Tanh Gradient:\n",
      " [[0.41997434 0.07065082 0.00986604]\n",
      " [0.41997434 0.07065082 0.00986604]\n",
      " [0.78644773 1.         0.78644773]]\n",
      "\n",
      "Softmax Activation Output:\n",
      " [[0.09003057 0.24472847 0.66524096]\n",
      " [0.66524096 0.24472847 0.09003057]\n",
      " [0.50648039 0.30719589 0.18632372]]\n",
      "\n",
      "Softmax Output Sum (should be 1 for each row):\n",
      " [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from si.neural_networks.activation import TanhActivation, SoftmaxActivation\n",
    "\n",
    "# Create some sample inputs\n",
    "inputs = np.array([[1.0, 2.0, 3.0], [-1.0, -2.0, -3.0], [0.5, 0.0, -0.5]])\n",
    "\n",
    "# Tanh Activation Test\n",
    "tanh_layer = TanhActivation()\n",
    "tanh_output = tanh_layer.forward_propagation(inputs, training=True)\n",
    "tanh_gradient = tanh_layer.backward_propagation(np.ones_like(inputs))\n",
    "\n",
    "print(\"\\nTanh Activation Output:\\n\", tanh_output)\n",
    "print(\"Tanh Gradient:\\n\", tanh_gradient)\n",
    "\n",
    "# Softmax Activation Test\n",
    "softmax_layer = SoftmaxActivation()\n",
    "softmax_output = softmax_layer.forward_propagation(inputs, training=True)\n",
    "\n",
    "print(\"\\nSoftmax Activation Output:\\n\", softmax_output)\n",
    "\n",
    "# Validate softmax outputs are probabilities\n",
    "print(\"\\nSoftmax Output Sum (should be 1 for each row):\\n\", np.sum(softmax_output, axis=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 14:** Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Cross Entropy:\n",
      "Loss: 0.3635480396729776\n",
      "Derivative: [[-0.41666667  0.          0.        ]\n",
      " [ 0.         -0.47619048  0.        ]\n",
      " [ 0.          0.         -0.55555556]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from si.neural_networks.losses import CategoricalCrossEntropy\n",
    "\n",
    "# Test data\n",
    "y_true_binary = np.array([0, 1, 1, 0])\n",
    "y_pred_binary = np.array([0.1, 0.9, 0.8, 0.2])\n",
    "\n",
    "y_true_categorical = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "y_pred_categorical = np.array([[0.8, 0.1, 0.1], [0.1, 0.7, 0.2], [0.2, 0.2, 0.6]])\n",
    "\n",
    "# Test Categorical Cross Entropy\n",
    "cce = CategoricalCrossEntropy()\n",
    "print(\"Categorical Cross Entropy:\")\n",
    "print(\"Loss:\", cce.loss(y_true_categorical, y_pred_categorical))\n",
    "print(\"Derivative:\", cce.derivative(y_true_categorical, y_pred_categorical))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 15:** Adams Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Adam:\n",
      "Updated weights: [0.99 1.99 2.99]\n",
      "Updated weights: [0.98 1.98 2.98]\n",
      "Updated weights: [0.97 1.97 2.97]\n",
      "Updated weights: [0.96 1.96 2.96]\n",
      "Updated weights: [0.95 1.95 2.95]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.optimizers import Adam\n",
    "\n",
    "# Test data\n",
    "w = np.array([1.0, 2.0, 3.0])\n",
    "grad_loss_w = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "# Test Adam\n",
    "print(\"Testing Adam:\")\n",
    "adam = Adam(learning_rate=0.01)\n",
    "for _ in range(5):\n",
    "    w = adam.update(w, grad_loss_w)\n",
    "    print(f\"Updated weights: {w}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
